"""
Tutorial 14: Streaming Agent with Server-Sent Events (SSE)

This agent demonstrates streaming responses using ADK's Server-Sent Events (SSE)
for real-time, progressive output that provides better user experience.
"""

import os
from typing import AsyncIterator, Dict, Any
from google.adk.agents import Agent
from google.adk.runners import Runner, LiveRequestQueue
from google.adk.agents.run_config import RunConfig, StreamingMode
from google.adk.sessions import InMemorySessionService
from google.genai import types


# Environment setup for Google AI
os.environ.setdefault('GOOGLE_GENAI_USE_VERTEXAI', 'FALSE')


def create_streaming_agent() -> Agent:
    """
    Create a streaming agent for real-time responses.

    Returns:
        Agent configured for streaming responses
    """
    return Agent(
        model='gemini-2.0-flash',
        name='streaming_assistant',
        description='Helpful assistant with streaming responses for real-time chat',
        instruction="""
You are a helpful, friendly assistant that provides detailed responses with streaming output.

Guidelines:
- Be conversational and engaging
- Provide detailed explanations when asked
- Ask clarifying questions if needed
- Remember conversation context
- Be concise for simple queries, detailed for complex ones
- Structure your responses clearly with sections when appropriate
        """.strip(),
        generate_content_config=types.GenerateContentConfig(
            temperature=0.7,  # Conversational
            max_output_tokens=2048,
            top_p=0.8,
            top_k=40
        )
    )


# Global agent instance
root_agent = create_streaming_agent()


async def stream_agent_response(query: str) -> AsyncIterator[str]:
    """
    Stream agent response to a query using ADK's actual streaming APIs.

    Args:
        query: User query to process

    Yields:
        Text chunks as they're generated by the AI model
    """
    # Create runner and session service
    session_service = InMemorySessionService()
    runner = Runner(app_name="streaming_agent", agent=root_agent, session_service=session_service)

    # Create a session for this conversation
    session = await session_service.create_session(
        app_name="streaming_agent",
        user_id="demo_user"
    )

    # Configure for SSE streaming
    run_config = RunConfig(
        streaming_mode=StreamingMode.SSE,
        max_llm_calls=50
    )

    try:
        # Run the agent with streaming using run_async
        async for event in runner.run_async(
            user_id="demo_user",
            session_id=session.id,
            new_message=types.Content(role="user", parts=[types.Part(text=query)]),
            run_config=run_config
        ):
            # Handle different event types
            if event.content and event.content.parts:
                for part in event.content.parts:
                    if part.text:
                        # Yield the text chunk
                        yield part.text

            # Check for completion
            if event.turn_complete:
                break

    except Exception as e:
        # Fallback to simulated streaming if real streaming fails
        print(f"Warning: Real streaming failed ({e}), falling back to simulation")
        mock_response = f"I understand you asked: '{query}'. This demonstrates streaming concepts through progressive text output."

        words = mock_response.split()
        for word in words:
            yield word + " "
            import asyncio
            await asyncio.sleep(0.01)


async def get_complete_response(query: str) -> str:
    """
    Get complete response (non-streaming) for testing or when streaming isn't needed.

    Args:
        query: User query

    Returns:
        Complete response text
    """
    # Create runner and session service
    session_service = InMemorySessionService()
    runner = Runner(app_name="streaming_agent", agent=root_agent, session_service=session_service)

    # Create a session
    session = await session_service.create_session(
        app_name="streaming_agent",
        user_id="demo_user"
    )

    # Configure for non-streaming
    run_config = RunConfig(
        streaming_mode=StreamingMode.NONE,
        max_llm_calls=50
    )

    # Collect all events
    response_parts = []

    async for event in runner.run_async(
        user_id="demo_user",
        session_id=session.id,
        new_message=types.Content(role="user", parts=[types.Part(text=query)]),
        run_config=run_config
    ):
        if event.content and event.content.parts:
            for part in event.content.parts:
                if part.text:
                    response_parts.append(part.text)

        if event.turn_complete:
            break

    return ''.join(response_parts)


def create_demo_session():
    """
    Create a demo session for testing.

    Returns:
        Session object for the demo
    """
    import asyncio
    session_service = InMemorySessionService()

    # Create session synchronously for demo purposes
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        session = loop.run_until_complete(
            session_service.create_session(
                app_name="streaming_agent",
                user_id="demo_user"
            )
        )
        return session
    finally:
        loop.close()


# Tool functions for enhanced functionality
def format_streaming_info() -> Dict[str, Any]:
    """
    Provide information about streaming capabilities.

    Returns:
        Dictionary with streaming information
    """
    return {
        'status': 'success',
        'report': 'Streaming information retrieved successfully',
        'data': {
            'streaming_modes': ['SSE', 'BIDI', 'OFF'],
            'current_mode': 'SSE',
            'benefits': [
                'Real-time user feedback',
                'Better perceived performance',
                'Progressive output display',
                'Early response interruption'
            ],
            'use_cases': [
                'Chat applications',
                'Long-form content generation',
                'Interactive assistants',
                'Real-time analysis'
            ]
        }
    }


def analyze_streaming_performance(query_length: int = 100) -> Dict[str, Any]:
    """
    Analyze streaming performance characteristics.

    Args:
        query_length: Length of query to analyze

    Returns:
        Performance analysis data
    """
    try:
        # Simulate performance analysis
        estimated_chunks = max(1, query_length // 50)  # Rough estimate
        estimated_time = query_length * 0.1  # Rough time estimate

        return {
            'status': 'success',
            'report': f'Performance analysis for query length {query_length}',
            'data': {
                'estimated_chunks': estimated_chunks,
                'estimated_total_time_seconds': estimated_time,
                'chunk_size_range': '10-100 characters',
                'recommended_buffer_size': 1024,
                'memory_efficient': True
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'report': 'Failed to analyze streaming performance'
        }


# Add tools to agent
root_agent.tools = [format_streaming_info, analyze_streaming_performance]